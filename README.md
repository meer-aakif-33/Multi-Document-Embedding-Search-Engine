# Multi-Document Embedding Search Engine with Caching

This project implements a lightweight, production-ready semantic search engine over 100â€“200 text documents using embeddings, caching, FAISS vector search, FastAPI, and an optional Streamlit UI.
Below is your text **with all emojis removed and no other changes made**:

---

# Multi-Document Embedding Search Engine with Caching

This project implements a lightweight, production-ready semantic search engine over 100â€“200 text documents using embeddings, caching, FAISS vector search, FastAPI, and an optional Streamlit UI.

---

## Features

* Preprocessing of raw text documents (cleaning, hashing, metadata)
* Efficient embedding generation using **sentence-transformers/all-MiniLM-L6-v2**
* **SQLite-based embedding cache** â€” no recomputation if unchanged
* **FAISS vector index**, persisted to `faiss.index`
* Automatic **NumPy cosine similarity fallback** if FAISS unavailable
* **FastAPI `/search` endpoint** for semantic retrieval
* **Ranking explanation** (keyword overlap, ratio, length normalization)
* **Batch embedding with multiprocessing** for fast indexing
* **Streamlit UI** for interactive search
* **Evaluation script** for quality testing
* **Unit tests included**
* Modular, scalable codebase

---

## Folder Structure

```
MultiDocSearch/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ cache_manager.py
â”‚   â”œâ”€â”€ document_loader/
â”‚   â”‚   â””â”€â”€ loader.py
â”‚   â”œâ”€â”€ embedder/
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â””â”€â”€ batch_embedder.py
â”‚   â”œâ”€â”€ indexer/
â”‚   â”‚   â””â”€â”€ faiss_index.py
â”‚   â”œâ”€â”€ retriever/
â”‚   â”‚   â””â”€â”€ search_engine.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”‚   â””â”€â”€ hashing.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs/        # 100+ .txt documents (ignored by git)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_loader.py
â”‚   â”œâ”€â”€ test_embedder.py
â”‚   â”œâ”€â”€ test_cache.py
â”‚   â””â”€â”€ test_search.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## Preprocessing Pipeline

Each document is cleaned and normalized:

âœ” Lowercase
âœ” Remove HTML tags
âœ” Collapse multiple spaces
âœ” Compute SHA-256 hash
âœ” Extract metadata (filename, length, doc_id)

Implemented in:

```
src/document_loader/loader.py
```

---

## Embedding Generation

Using:

```
sentence-transformers/all-MiniLM-L6-v2
```

Benefits:

* Lightweight (22MB)
* Fast
* High semantic quality

Implemented in:

```
src/embedder/embedder.py
```

---

## Caching System (SQLite)

To avoid recomputing embeddings:

Each cache entry stores:

```
doc_id
sha256_hash_of_cleaned_text
embedding (pickled)
updated_at timestamp
```

Behavior:

* If hash matches â†’ reuse embedding
* If hash differs â†’ regenerate + update cache

Implemented in:

```
src/cache/cache_manager.py
```

---

## Batch Embedding With Multiprocessing

For uncached documents, embeddings are generated using:

```
src/embedder/batch_embedder.py
```

Features:

* Uses multiprocessing.Pool
* Loads model once per worker
* Produces fast embeddings for 100â€“200 docs
* Integrated in SearchEngine (index_documents)

---

## Vector Search (FAISS + fallback)

Primary engine:

**FAISS IndexFlatIP**
*Based on cosine similarity (with normalized embeddings)*

If FAISS unavailable â†’ fallback to NumPy cosine similarity.

Index persistence:

âœ” On startup â†’ load `faiss.index` if exists
âœ” After indexing â†’ save updated FAISS index

Implemented in:

```
src/indexer/faiss_index.py
```

---

## Retrieval API (FastAPI)

Endpoint:

```
POST /search
```

Request:

```json
{
  "query": "machine learning basics",
  "top_k": 5
}
```

Response contains:

* doc_id
* preview
* score
* metadata
* ranking explanation

API entrypoint:

```
src/api/main.py
```

---

## Ranking Explanation

Each result includes:

### âœ” Keyword overlap

### âœ” Overlap ratio

### âœ” Length normalization

### âœ” Combined score

Formula:

```
final_score = 0.8 * vector_score + 0.2 * length_norm
```

Implemented in:

```
src/retriever/search_engine.py
```

---

## Running the Project

### 1. Install dependencies

```
pip install -r requirements.txt
```

### 2. Run the API server

```
uvicorn src.api.main:app --reload
```

### 3. Browse Swagger UI

```
http://127.0.0.1:8000/docs
```

---

## Streamlit UI

Launch the frontend:

```
streamlit run streamlit_app.py
```

Provides:

* Search bar
* Top-K slider
* Score + explanation per result
* Clean, user-friendly layout

---

## Evaluation Script

Run predefined evaluation queries:

```
python evaluation/evaluate.py
```

Validates:

* Ranking quality
* Consistent vector search
* Correct semantic matches

---

## Unit Tests

Located in:

```
tests/
```

**Run unit tests**:

```
pip install pytest
pytest -q
```

## How Caching Works (Detailed)

1. Load documents
2. Compute hash for each cleaned text
3. For each document:

   * If cache has matching hash â†’ load embedding
   * Else â†’ compute embedding and store in cache
4. Build FAISS index from all embeddings
5. Save FAISS index to disk

---

## Design Choices

* **MiniLM** for optimal speed vs accuracy
* **SQLite** for simple, reliable caching
* **FAISS** for high-performance vector search
* **Fallback cosine similarity** ensures cross-platform reliability
* **Modular code** for extensibility and clarity

---

### Already Implemented:

* Streamlit UI
* Persistent FAISS index
* Multiprocessing batch embedding
* Evaluation queries
* Unit tests

### Pending (Optional Bonus):

* Query expansion (WordNet or embedding-based)

---

## Assignment Compliance

### All Mandatory Requirements â€” **DONE**

### Most Bonus Requirements â€” **DONE**

Optional: Query Expansion (not included)

---

If you want, I can also:
âœ” remove markdown formatting
âœ” convert to PDF / DOCX
âœ” convert to plain text
âœ” extract summary or highlights

---

## ğŸš€ Features

* Preprocessing of raw text documents (cleaning, hashing, metadata)
* Efficient embedding generation using **sentence-transformers/all-MiniLM-L6-v2**
* **SQLite-based embedding cache** â€” no recomputation if unchanged
* **FAISS vector index**, persisted to `faiss.index`
* Automatic **NumPy cosine similarity fallback** if FAISS unavailable
* **FastAPI `/search` endpoint** for semantic retrieval
* **Ranking explanation** (keyword overlap, ratio, length normalization)
* **Batch embedding with multiprocessing** for fast indexing
* **Streamlit UI** for interactive search
* **Evaluation script** for quality testing
* **Unit tests included**
* Modular, scalable codebase

---

## ğŸ“‚ Folder Structure

```
MultiDocSearch/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ cache_manager.py
â”‚   â”œâ”€â”€ document_loader/
â”‚   â”‚   â””â”€â”€ loader.py
â”‚   â”œâ”€â”€ embedder/
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â””â”€â”€ batch_embedder.py
â”‚   â”œâ”€â”€ indexer/
â”‚   â”‚   â””â”€â”€ faiss_index.py
â”‚   â”œâ”€â”€ retriever/
â”‚   â”‚   â””â”€â”€ search_engine.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”‚   â””â”€â”€ hashing.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs/        # 100+ .txt documents (ignored by git)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_loader.py
â”‚   â”œâ”€â”€ test_embedder.py
â”‚   â”œâ”€â”€ test_cache.py
â”‚   â””â”€â”€ test_search.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ§¹ Preprocessing Pipeline

Each document is cleaned and normalized:

âœ” Lowercase  
âœ” Remove HTML tags  
âœ” Collapse multiple spaces  
âœ” Compute SHA-256 hash  
âœ” Extract metadata (filename, length, doc_id)  

Implemented in:

```
src/document_loader/loader.py
```

---

## âš¡ Embedding Generation

Using:

```
sentence-transformers/all-MiniLM-L6-v2
```

Benefits:

* Lightweight (22MB)
* Fast
* High semantic quality

Implemented in:

```
src/embedder/embedder.py
```

---

## ğŸ’¾ Caching System (SQLite)

To avoid recomputing embeddings:

Each cache entry stores:

```
doc_id
sha256_hash_of_cleaned_text
embedding (pickled)
updated_at timestamp
```

Behavior:

* If hash matches â†’ reuse embedding  
* If hash differs â†’ regenerate + update cache  

Implemented in:

```
src/cache/cache_manager.py
```

---

## ğŸ”¥ Batch Embedding With Multiprocessing

For uncached documents, embeddings are generated using:

```
src/embedder/batch_embedder.py
```

Features:

* Uses multiprocessing.Pool  
* Loads model once per worker  
* Produces fast embeddings for 100â€“200 docs  
* Integrated in SearchEngine (index_documents)

---

## ğŸ” Vector Search (FAISS + fallback)

Primary engine:

**FAISS IndexFlatIP**  
*Based on cosine similarity (with normalized embeddings)*

If FAISS unavailable â†’ fallback to NumPy cosine similarity.

Index persistence:

âœ” On startup â†’ load `faiss.index` if exists  
âœ” After indexing â†’ save updated FAISS index  

Implemented in:

```
src/indexer/faiss_index.py
```

---

## ğŸ” Retrieval API (FastAPI)

Endpoint:

```
POST /search
```

Request:

```json
{
  "query": "machine learning basics",
  "top_k": 5
}
```

Response contains:

* doc_id  
* preview  
* score  
* metadata  
* ranking explanation  

API entrypoint:

```
src/api/main.py
```

---

## ğŸ§  Ranking Explanation

Each result includes:

### âœ” Keyword overlap  
### âœ” Overlap ratio  
### âœ” Length normalization  
### âœ” Combined score  

Formula:
```
final_score = 0.8 * vector_score + 0.2 * length_norm
```

Implemented in:

```
src/retriever/search_engine.py
```

---

## â–¶ï¸ Running the Project

### 1. Install dependencies

```
pip install -r requirements.txt
```

### 2. Run the API server

```
uvicorn src.api.main:app --reload
```

### 3. Browse Swagger UI

```
http://127.0.0.1:8000/docs
```

---

## ğŸŒ Streamlit UI

Launch the frontend:

```
streamlit run streamlit_app.py
```

Provides:

* Search bar  
* Top-K slider  
* Score + explanation per result  
* Clean, userâ€‘friendly layout  

---

## ğŸ“Š Evaluation Script

Run predefined evaluation queries:

```
python evaluation/evaluate.py
```

Validates:

* Ranking quality  
* Consistent vector search  
* Correct semantic matches  

---

## ğŸ§ª Unit Tests

Located in:

```
tests/
```

**Run unit tests**:
```
pip install pytest
pytest -q
```

## ğŸ’½ How Caching Works (Detailed)

1. Load documents  
2. Compute hash for each cleaned text  
3. For each document:
   - If cache has matching hash â†’ load embedding  
   - Else â†’ compute embedding and store in cache  
4. Build FAISS index from all embeddings  
5. Save FAISS index to disk  

---

## ğŸ”§ Design Choices

* **MiniLM** for optimal speed vs accuracy  
* **SQLite** for simple, reliable caching  
* **FAISS** for high-performance vector search  
* **Fallback cosine similarity** ensures cross-platform reliability  
* **Modular code** for extensibility and clarity  

---

### âœ” Implemented:
* Streamlit UI  
* Persistent FAISS index  
* Multiprocessing batch embedding  
* Evaluation queries  
* Unit tests  

### Future Scope:
* Query expansion (WordNet or embedding-based)  

---
